{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Nets\n",
    "\n",
    "## Related Work\n",
    "* Undirected graphical models with latent variables\n",
    "    * Restricted Boltzmann Machines\n",
    "    * Deep Boltzmann Machines\n",
    "* Deep Belief Networks\n",
    "* Noise-Contrastive Estimation\n",
    "* Generative Stochastic Network\n",
    "* Variational Auto Encoders\n",
    "* Stochastic Backpropagation\n",
    "\n",
    "## Adversarial Nets\n",
    "* Goal: Learn distribution $p_g$ over data $x$.\n",
    "* Method:\n",
    "    * Prior on input variables $p_z(z)$\n",
    "    * Map $z \\rightarrow G(z; \\theta_g)$. G is differentiable.\n",
    "    * Map $x \\rightarrow D(x; \\theta_d)$.\n",
    "    * Train:\n",
    "        * D to maximize the probability of classifying correctly its inputs (real or fake).\n",
    "        * G to minimize $\\log(1-D(G(z)))$.\n",
    "        * In short: $\\min\\limits_{G}\\max\\limits_{D} V(G, D) = \\mathbb{E}_{x\\sim p_{data}(x)} [\\log D(x)] + \\mathbb{E}_{z\\sim p_z(z)}[\\log(1-D(G(z)))]$\n",
    "        * It is preferable to train G by maximizing $\\log D(G(z))$ to obtain stronger gradients at the beginning (by avoiding saturation).\n",
    "* After several steps, the equilibrium (neither can make improvements) will be reached and $p_g = p_{data}$ (given that both networks have enough capacity).\n",
    "\n",
    "## Theoretical Results\n",
    "*This assumes models with infinite capacity in order to study convergence in the space of probability density functions.*\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{align}\n",
    "V(G, D) &= \\mathbb{E}_{x\\sim p_{data}(x)} [\\log D(x)] + \\mathbb{E}_{z\\sim p_z(z)}[\\log(1-D(G(z)))] \\nonumber\\\\\n",
    "\n",
    "&= \\mathbb{E}_{x\\sim p_{data}(x)} [\\log D(x)] + \\mathbb{E}_{x\\sim p_g(x)}[\\log(1-D(x))] \\nonumber\\\\\n",
    "\n",
    "&= \\int_{x}p_{data}(x)\\log(D(x)) + p_g(x)\\log(1-D(x))dx \\nonumber\n",
    "\\end{align}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "$(a, b)\\in \\mathbb{R}^2 \\{0, 0\\},\\ y\\rightarrow a\\log(y) + b\\log(1-y)$ achieves its maximum in $[0,1]$ at $\\frac{a}{a+b}$ (derivative w.r.t. $y$ and make it $=0$). The discriminator does not need to be defined outside of $Supp(p_{data}) \\cup Supp(p_{g})$ i.e. when $(a,b) = (0,0)$\n",
    "\n",
    "$$C(G) = \\max\\limits_DV(G, D)$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
