{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/userr2232/deeplearning-notes/blob/master/generative_adversarial_nets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8njkvJixqWPI"
      },
      "source": [
        "# Generative Adversarial Nets\n",
        "\n",
        "## Related Work\n",
        "* Undirected graphical models with latent variables\n",
        "    * Restricted Boltzmann Machines\n",
        "    * Deep Boltzmann Machines\n",
        "* Deep Belief Networks\n",
        "* Noise-Contrastive Estimation\n",
        "* Generative Stochastic Network\n",
        "* Variational Auto Encoders\n",
        "* Stochastic Backpropagation\n",
        "\n",
        "## Adversarial Nets\n",
        "* Goal: Learn distribution $p_g$ over data $x$.\n",
        "* Method:\n",
        "    * Prior on input variables $p_z(z)$\n",
        "    * Map $z \\rightarrow G(z; \\theta_g)$. G is differentiable.\n",
        "    * Map $x \\rightarrow D(x; \\theta_d)$.\n",
        "    * Train:\n",
        "        * D to maximize the probability of classifying correctly its inputs (real or fake).\n",
        "        * G to minimize $\\log(1-D(G(z)))$.\n",
        "        * In short: $\\min\\limits_{G}\\max\\limits_{D} V(G, D) = \\mathbb{E}_{x\\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z\\sim p_z(z)}[\\log(1-D(G(z)))]$\n",
        "        * It is preferable to train G by maximizing $\\log D(G(z))$ to obtain stronger gradients at the beginning (by avoiding saturation).\n",
        "* After several steps, the equilibrium (neither can make improvements) will be reached and $p_g = p_{data}$ (given that both networks have enough capacity).\n",
        "\n",
        "## Theoretical Results\n",
        "*This assumes models with infinite capacity in order to study convergence in the space of probability density functions.*\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\begin{align}\n",
        "V(G, D) &= \\mathbb{E}_{x\\sim p_{data}(x)} [\\log D(x)] + \\mathbb{E}_{z\\sim p_z(z)}[\\log(1-D(G(z)))] \\\\\n",
        "\n",
        "&= \\mathbb{E}_{x\\sim p_{data}(x)} [\\log D(x)] + \\mathbb{E}_{x\\sim p_g(x)}[\\log(1-D(x))] \\\\\n",
        "\n",
        "&= \\int_{x}p_{data}(x)\\log(D(x)) + p_g(x)\\log(1-D(x))dx\n",
        "\\end{align}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "\n",
        "$(a, b)\\in \\mathbb{R}^2 \\{0, 0\\},\\ y\\rightarrow a\\log(y) + b\\log(1-y)$ achieves its maximum in $[0,1]$ at $\\frac{a}{a+b}$ (derivative w.r.t. $y$ and make it $=0$). The discriminator does not need to be defined outside of $Supp(p_{data}) \\cup Supp(p_{g})$ i.e. when $(a,b) = (0,0)$\n",
        "\n",
        "$$C(G) = \\max\\limits_DV(G, D)$$"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "plaintext"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "generative-adversarial-nets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}